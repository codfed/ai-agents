{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending Wikipedia articles using Langchain Memory to add context\n",
    "\n",
    "When I have run the trending analysis in my previous notebooks, I was seeing articles that related to each other, but one didn't have any new information present in the portion of the artcile I was passing.\n",
    "\n",
    "For instance, Jeff Baena passed away. His article trended and his recent death was correctly identified as the reason. His wife, Aubrey Plaza, had her article trending as well. But the reason for her trending article was vague and did not recognize her husband's death as the reason.\n",
    "\n",
    "I am testing out Langchain's memory feature to try to solve this problem\n",
    "\n",
    "This current iteration filters out the articles that have already been trending. This makes for a more interesting list. I've also implemented a master HTML file that keeps track of every time this is run.\n",
    "\n",
    "## Takeaways\n",
    "- Very pleased with the newly refined prompt for the relation to other articles.\n",
    "- It's important to distiguish which problems are better solved with a function vs an LLM. My though right now is that the more structured data is, the more likely a static function is the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-openai langchain-community openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get trending wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "yesterday = today - datetime.timedelta(days=1)\n",
    "\n",
    "date_to_query = yesterday\n",
    "url = f\"https://api.wikimedia.org/feed/v1/wikipedia/en/featured/{date_to_query.strftime('%Y/%m/%d')}\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "featured_feed = response.json()\n",
    "print(f\"API call: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Ensure the 'featured-feed' folder exists\n",
    "file_directory = \"data\"\n",
    "os.makedirs(file_directory, exist_ok=True)\n",
    "\n",
    "# Define the filename based on the date\n",
    "base_file_name = date_to_query.strftime('%Y-%m-%d')\n",
    "file_path = f'{file_directory}/{base_file_name}.json'\n",
    "\n",
    "# Save to JSON file (overwrite if it already exists)\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(featured_feed, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f'Saved Wikipedia response to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data structure with all relevant information and placeholders for LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "\n",
    "\n",
    "for item in featured_feed['mostread']['articles']:\n",
    "    title = item['title']\n",
    "    normalized_title = item['titles']['normalized']\n",
    "    views = item['views']\n",
    "    link = item['content_urls']['desktop']['page']\n",
    "    extract = item['extract']\n",
    "    thumbnail = item.get('thumbnail', {}).get('source', None)\n",
    "    view_history = item['view_history']\n",
    "\n",
    "    article={\n",
    "        'title': title,\n",
    "        'normalized_title': normalized_title,\n",
    "        'views': views,\n",
    "        'link': link,\n",
    "        'thumbnail': thumbnail,\n",
    "        'extract': extract,\n",
    "        'text': '',\n",
    "        'trendingreason': '',\n",
    "        'memorycontext': '',\n",
    "        'view_history': view_history,\n",
    "        'is_newly_trending': ''\n",
    "    }\n",
    "\n",
    "    article_list.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out the already trending articles\n",
    "\n",
    "- These results were erratic. So I went with the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the LangChain components\n",
    "# chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Set temperature to 0 for deterministic results\n",
    "\n",
    "# conversation_chain = ConversationChain(llm=chat_model, verbose=True)\n",
    "\n",
    "# def is_newly_trending(view_history):\n",
    "#     # Format view history as a string for the prompt\n",
    "#     formatted_history = \"\\n\".join(\n",
    "#         [f\"- {entry['date']}: {entry['views']} views\" for entry in view_history]\n",
    "#     )\n",
    "    \n",
    "#     # Construct the full input question\n",
    "#     prompt = f\"\"\"\n",
    "#     Given the following view history data:\n",
    "#     {formatted_history}\n",
    "    \n",
    "#     A data point is considered to be trending if the final day shows a meaningful spike.\n",
    "    \n",
    "#     A \"meaningful spike\" means the views on the final day provided have \n",
    "#     increased significantly relative the previous day.\n",
    "    \n",
    "#     Respond with only \"true\" or \"false\" without any additional explanation or text.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Run the chain\n",
    "#     response = conversation_chain.predict(input=prompt)\n",
    "#     print(f\"CODYBUG: response: {response}\")\n",
    "#     return response.strip().lower() == \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if the article is newly trending. If it is, add to new list\n",
    "\n",
    "- If the article views increased by a factor of 5 from the previous day I'm calling newly trending\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_newly_trending(view_history):\n",
    "    view_history_length = len(view_history)\n",
    "\n",
    "    yesterdays_views = view_history[view_history_length-2]['views']\n",
    "    todays_views = view_history[view_history_length-1]['views']\n",
    "\n",
    "    return todays_views*0.02 > yesterdays_views\n",
    "\n",
    "newly_trending_article_list = []\n",
    "\n",
    "for article in article_list:\n",
    "    newly_trending = is_newly_trending(article['view_history'])\n",
    "    article['is_newly_trending'] = newly_trending\n",
    "    \n",
    "    if newly_trending:\n",
    "        newly_trending_article_list.append(article)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in newly_trending_article_list:\n",
    "    print(article['title'])\n",
    "    print(article['is_newly_trending'])\n",
    "    print(article['view_history'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get first 5000 characters of article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in newly_trending_article_list:\n",
    "      # Download raw text of article\n",
    "  url = f\"https://en.wikipedia.org/w/index.php?title={article['title']}&action=raw\"\n",
    "  print(url)\n",
    "\n",
    "  article_text = requests.get(url).text\n",
    "  article_text_truncated = article_text[:5000]\n",
    "  article['text'] =  article_text_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating conversation chain with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful Wikipedia analyst and historian. \n",
    "            You speak consiseley and given the choice to say too much or too little, you say too little\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "trending_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=trending_prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through all articles in data structure\n",
    "- Use LangChain/ChatGPT to give suggestions why each one is trending\n",
    "- Save reason to structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for article in newly_trending_article_list:\n",
    "    print(f\"Analyzing {title}\")\n",
    "\n",
    "    title = article['title']\n",
    "    text = article['text']\n",
    "\n",
    "    prediction_prompt = f\"\"\"Act as a professional news summarizer. Based on your knowledge of {title} \n",
    "    and the following extract. In 1 concise and confident sentence, explain why the {title} \n",
    "    article might be trending on Wikipedia on #{date_to_query}:\\n\\n{text}\"\"\"\n",
    "\n",
    "    response = trending_conversation.predict(input=prediction_prompt)\n",
    "    print(\"trendingreason:\", response)\n",
    "    \n",
    "    article['trendingreason'] =  response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use conversation memory to derive more context from\n",
    "\n",
    "- Pass memory from first conversation into a new conversation \n",
    "- Search for cross context between today's articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful Wikipedia analyst and historian. \n",
    "            You speak consiseley and given the choice to say too much or too little, you say too little.\n",
    "            If you do not know something, you say so.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# todays_memory = load_memory()\n",
    "memory_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=trending_conversation.memory,\n",
    "    prompt=memory_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_miss_response = \"False\"\n",
    "\n",
    "for article in newly_trending_article_list[:1]:\n",
    "    print(f\"Analyzing {title}\")\n",
    "\n",
    "    title = article['title']\n",
    "    text = article['text']\n",
    "\n",
    "    memory_prompt = f\"\"\"Does {title} relate to any other trending article from today?\n",
    "     If it does, give me a short description of the relation. If it does not, reply with '{llm_miss_response}'\"\"\"\n",
    "\n",
    "    response = memory_conversation.predict(input=memory_prompt)\n",
    "    print(\"memorycontext:\", response)\n",
    "    \n",
    "    article['memorycontext'] =  response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newly_trending_article_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build HTML Page to display the top 10 list complete with \n",
    "- title\n",
    "- thumbnail\n",
    "- trending reason\n",
    "- relation to other articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start building the HTML\n",
    "html_title = f\"<h1>Newly Trending on {date_to_query.strftime(\"%B %d, %Y\")}</h1>\\n\"\n",
    "if len(newly_trending_article_list) > 0:\n",
    "    html_list = \"<ol>\\n\"\n",
    "\n",
    "    # Iterate through the data\n",
    "    for item in newly_trending_article_list:\n",
    "        title = item['normalized_title']\n",
    "        link = item['link']\n",
    "        thumbnail = item['thumbnail']\n",
    "        trendingreason = item['trendingreason']\n",
    "        \n",
    "        memorycontext = item['memorycontext']\n",
    "        extract = item['extract']\n",
    "\n",
    "        # Handle null thumbnail\n",
    "        if thumbnail:\n",
    "            thumbnail_html = f'<img src=\"{thumbnail}\" alt=\"Thumbnail for {title}\"/><br>'\n",
    "        else:\n",
    "            thumbnail_html = ''\n",
    "        \n",
    "\n",
    "        # Handle relation to others prompt returning a miss, \n",
    "        sanitized_memorycontext = memorycontext.strip().rstrip('.').lower()\n",
    "\n",
    "        if sanitized_memorycontext == llm_miss_response.lower():\n",
    "            relation_output = ''\n",
    "        else:\n",
    "            relation_output = f\"<strong>Relation to other trending articles:</strong> {memorycontext}\"\n",
    "\n",
    "        # Create a list item for each entry\n",
    "        html_list += f\"\"\"\n",
    "        <li>\n",
    "            <h2>\n",
    "            <a href=\"{link}\" target=\"_blank\">{title}</a><br>\n",
    "            </h2>\n",
    "            {thumbnail_html}\n",
    "            <strong>Views:</strong> {views}<br><br>\n",
    "            <strong>Reason for Trending:</strong> {trendingreason}<br><br>\n",
    "            {relation_output}\n",
    "        </li>\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    # Close the HTML list\n",
    "    html_list += \"\\n</ol>\"\n",
    "else:\n",
    "    html_list = \"<p>No articles are trending today.</p>\"\n",
    "html_page = html_title + html_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'data' folder exists\n",
    "file_directory = \"data\"\n",
    "os.makedirs(file_directory, exist_ok=True)\n",
    "\n",
    "# Define the filename based on the date\n",
    "base_file_name = date_to_query.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save to html file (overwrite if it already exists)\n",
    "file_path = f'{file_directory}/{base_file_name}.html'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(html_page)\n",
    "\n",
    "# Prepend to the master file\n",
    "master_file_path = f'{file_directory}/master.html'\n",
    "\n",
    "# Read the existing content of the master file if it exists\n",
    "if os.path.exists(master_file_path):\n",
    "    with open(master_file_path, 'r', encoding='utf-8') as master_file:\n",
    "        master_content = master_file.read()\n",
    "else:\n",
    "    master_content = ''\n",
    "\n",
    "# Combine the new content with the old master content\n",
    "updated_master_content = html_page + '\\n' + master_content\n",
    "\n",
    "# Save the updated content back to the master file\n",
    "with open(master_file_path, 'w', encoding='utf-8') as master_file:\n",
    "    master_file.write(updated_master_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display generated html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the HTML in the notebook (assuming Jupyter or similar)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(updated_master_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
