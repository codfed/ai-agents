{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending Wikipedia\n",
    "\n",
    "1. Get trending Wikipedia articles from yesterday\n",
    "2. Pass plain text from article to OpenAI for suggestsions as to why each article is trending\n",
    "3. Build HTML page to display each article and why it is trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests ollama tiktoken openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API call for the featured feed shows different for today than it does for any previous days.\n",
    "\n",
    "This is written for previous days only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API call: https://api.wikimedia.org/feed/v1/wikipedia/en/featured/2024/12/28\n",
      "Retrieved Wikipedia top article statistics for 2024-12-28 17:12:37.093627\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "yesterday = today - datetime.timedelta(days=1)\n",
    "\n",
    "date_to_query = yesterday\n",
    "url = 'https://api.wikimedia.org/feed/v1/wikipedia/en/featured/' + date_to_query.strftime('%Y/%m/%d')\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "featured_feed = response.json()\n",
    "print(f\"API call: {url}\")\n",
    "print(f\"Retrieved Wikipedia top article statistics for {date_to_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save API response to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Wikipedia response to data/2024-12-28.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Ensure the 'featured-feed' folder exists\n",
    "file_directory = \"data\"\n",
    "os.makedirs(file_directory, exist_ok=True)\n",
    "\n",
    "# Define the filename based on the date\n",
    "base_file_name = date_to_query.strftime('%Y-%m-%d')\n",
    "file_path = f'{file_directory}/{base_file_name}.json'\n",
    "\n",
    "# Save to JSON file (overwrite if it already exists)\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(featured_feed, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f'Saved Wikipedia response to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep an eye on the token count\n",
    "\n",
    "Since we're using the context window for the entire wikipedia article I want to keep an eye on the token count for each article. Here's what I've seen:\n",
    "\n",
    "- Squid_Game_season_2 (16k)\n",
    "- Olivia_Hussey (12k)\n",
    "- Greg_Gumbel (6k)\n",
    "- Bryant_Gumbel (8k)\n",
    "- Nosferatu_(2024_film) (17k)\n",
    "- Pushpa_2 (38k)\n",
    "- Manmohan_Singh (38k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing from Ollama to openai...\n",
    "\n",
    "When passing the entire article text to Ollama I was having a greate deal of hallucinations. Decided to see what it looked like to pass the entire text to ChatGPT. Dropped it down to do only the top article to test out the token count and cost\n",
    "\n",
    "\n",
    "- gpt-3.5-turbo-0125 16,385 tokens is not enough for the Anthropology article with 26k tokens\n",
    "- gpt-4-32k-0613 has a limit of 32k\n",
    "- gpt-4-turbo 128k tokens: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Manmohan_Singh\n",
      "https://en.wikipedia.org/w/index.php?title=Manmohan_Singh&action=raw\n",
      "Token count: 31993\n",
      "response: ChatCompletion(id='chatcmpl-AkL6GiKG7OY8FXNmnavSUK5uilolY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The article about Manmohan Singh, former Prime Minister of India, may be trending on Wikipedia due to his recent passing on December 26, 2024. His significant contributions as a key architect in liberalizing India's economy and his tenure as a respected global statesman are likely driving interest in his life and legacy.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735607560, model='gpt-4-turbo-2024-04-09', object='chat.completion', service_tier=None, system_fingerprint='fp_1a5512f3de', usage=CompletionUsage(completion_tokens=66, prompt_tokens=32064, total_tokens=32130, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "trendingreason: The article about Manmohan Singh, former Prime Minister of India, may be trending on Wikipedia due to his recent passing on December 26, 2024. His significant contributions as a key architect in liberalizing India's economy and his tenure as a respected global statesman are likely driving interest in his life and legacy.\n",
      "Analyzing Squid_Game_season_2\n",
      "https://en.wikipedia.org/w/index.php?title=Squid_Game_season_2&action=raw\n",
      "Token count: 16382\n",
      "response: ChatCompletion(id='chatcmpl-AkL6nlmZzTSkUQLQ2S6WnTV4uAQif', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The \"Squid Game season 2\" article might be trending on Wikipedia due to its recent release on December 26, 2024, sparking interest as viewers seek more detailed insights into the latest episodes, character developments, and plot twists of this popular dystopian series. Additionally, anticipation may be high for the confirmed upcoming third and final season.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735607593, model='gpt-4-turbo-2024-04-09', object='chat.completion', service_tier=None, system_fingerprint='fp_1a5512f3de', usage=CompletionUsage(completion_tokens=72, prompt_tokens=16453, total_tokens=16525, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "trendingreason: The \"Squid Game season 2\" article might be trending on Wikipedia due to its recent release on December 26, 2024, sparking interest as viewers seek more detailed insights into the latest episodes, character developments, and plot twists of this popular dystopian series. Additionally, anticipation may be high for the confirmed upcoming third and final season.\n",
      "Analyzing Nosferatu_(2024_film)\n",
      "https://en.wikipedia.org/w/index.php?title=Nosferatu_(2024_film)&action=raw\n",
      "Token count: 16654\n",
      "response: ChatCompletion(id='chatcmpl-AkL7GvtUMriHGIvT3Hk1GV6bCik4C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The \"Nosferatu\" (2024) film article likely is trending on Wikipedia due to its recent release in the United States on December 25, 2024, and its positive reception with both critics and audiences, as part of its theatrical rollout. The notable direction by Robert Eggers, compelling performances by its cast, and innovative retelling of the classic horror story contribute to the heightened interest and discussions around the film.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735607622, model='gpt-4-turbo-2024-04-09', object='chat.completion', service_tier=None, system_fingerprint='fp_1a5512f3de', usage=CompletionUsage(completion_tokens=87, prompt_tokens=16731, total_tokens=16818, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "trendingreason: The \"Nosferatu\" (2024) film article likely is trending on Wikipedia due to its recent release in the United States on December 25, 2024, and its positive reception with both critics and audiences, as part of its theatrical rollout. The notable direction by Robert Eggers, compelling performances by its cast, and innovative retelling of the classic horror story contribute to the heightened interest and discussions around the film.\n",
      "Analyzing Squid_Game\n",
      "https://en.wikipedia.org/w/index.php?title=Squid_Game&action=raw\n",
      "Token count: 34999\n",
      "response: ChatCompletion(id='chatcmpl-AkL8DdfcX4j0KO3i8o66otdgQRRqg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The Squid Game article is likely trending on Wikipedia today due to the recent release of the show's second season on Netflix on December 26, 2024, which has attracted significant viewer interest and engagement. This widespread attention typically drives fans and new viewers to seek more detailed information about the series, its characters, and production details.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735607681, model='gpt-4-turbo-2024-04-09', object='chat.completion', service_tier=None, system_fingerprint='fp_1a5512f3de', usage=CompletionUsage(completion_tokens=68, prompt_tokens=35064, total_tokens=35132, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "trendingreason: The Squid Game article is likely trending on Wikipedia today due to the recent release of the show's second season on Netflix on December 26, 2024, which has attracted significant viewer interest and engagement. This widespread attention typically drives fans and new viewers to seek more detailed information about the series, its characters, and production details.\n",
      "Analyzing Ainsley_Earhardt\n",
      "https://en.wikipedia.org/w/index.php?title=Ainsley_Earhardt&action=raw\n",
      "Token count: 7044\n",
      "response: ChatCompletion(id='chatcmpl-AkL8ULIjL90sJUQ2B4TE5SZUU61BV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Ainsley Earhardt article may be trending on Wikipedia due to the recent announcement of her engagement to fellow Fox News personality Sean Hannity on Christmas Day 2024, as highlighted in a significant update to her personal life details. This event has likely drawn increased public interest and media attention to her profile.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735607698, model='gpt-4-turbo-2024-04-09', object='chat.completion', service_tier=None, system_fingerprint='fp_1a5512f3de', usage=CompletionUsage(completion_tokens=62, prompt_tokens=7115, total_tokens=7177, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "trendingreason: The Ainsley Earhardt article may be trending on Wikipedia due to the recent announcement of her engagement to fellow Fox News personality Sean Hannity on Christmas Day 2024, as highlighted in a significant update to her personal life details. This event has likely drawn increased public interest and media attention to her profile.\n",
      "Analyzing Pushpa_2:_The_Rule\n",
      "https://en.wikipedia.org/w/index.php?title=Pushpa_2:_The_Rule&action=raw\n",
      "Token count: 38318\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-FzpucI54an60M5UmUwg4kb9M on tokens per min (TPM): Limit 30000, Requested 32212. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 30\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mToken count: \u001b[39m\u001b[39m{\u001b[39;00mnum_tokens_from_string(article_text,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcl100k_base\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAct as a professional news summarizer. Based on your knowledge of \u001b[39m\u001b[39m{\u001b[39;00mtitle\u001b[39m}\u001b[39;00m\u001b[39m and the following extract. In 1-2 sentences, explain why the \u001b[39m\u001b[39m{\u001b[39;00mtitle\u001b[39m}\u001b[39;00m\u001b[39m article might be trending on Wikipedia on #\u001b[39m\u001b[39m{\u001b[39;00mdate_to_query\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00marticle_text\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 30\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     31\u001b[0m model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     33\u001b[0m     {\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt\n\u001b[1;32m     36\u001b[0m     }\n\u001b[1;32m     37\u001b[0m ],\n\u001b[1;32m     38\u001b[0m temperature\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m max_tokens\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrendingreason: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    860\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    861\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    862\u001b[0m             {\n\u001b[1;32m    863\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    864\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    865\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    866\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    867\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    868\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    869\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    870\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    871\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    872\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    873\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    874\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    875\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    876\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    877\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    878\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    879\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[1;32m    880\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    881\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    882\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    883\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    884\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    885\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    886\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    887\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    888\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    889\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    890\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    891\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    892\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    893\u001b[0m             },\n\u001b[1;32m    894\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    895\u001b[0m         ),\n\u001b[1;32m    896\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    897\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    898\u001b[0m         ),\n\u001b[1;32m    899\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    900\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    902\u001b[0m     )\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    958\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    961\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    962\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1047\u001b[0m         input_options,\n\u001b[1;32m   1048\u001b[0m         cast_to,\n\u001b[1;32m   1049\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1050\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1051\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1052\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1053\u001b[0m     )\n\u001b[1;32m   1055\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1096\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1097\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1098\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1099\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1100\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1101\u001b[0m )\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1047\u001b[0m         input_options,\n\u001b[1;32m   1048\u001b[0m         cast_to,\n\u001b[1;32m   1049\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1050\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1051\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1052\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1053\u001b[0m     )\n\u001b[1;32m   1055\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1096\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1097\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1098\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1099\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1100\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1101\u001b[0m )\n",
      "File \u001b[0;32m~/devel/ai-agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4-turbo-preview in organization org-FzpucI54an60M5UmUwg4kb9M on tokens per min (TPM): Limit 30000, Requested 32212. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "articles_with_reasons = []\n",
    "\n",
    "\n",
    "for item in featured_feed['mostread']['articles'][:7]:\n",
    "    title = item['title']\n",
    "    views = item['views']\n",
    "    link = item['content_urls']['desktop']['page']\n",
    "    extract = item['extract']\n",
    "    thumbnail = item.get('thumbnail', {}).get('source', None)\n",
    "    print(f\"Analyzing {title}\")\n",
    "\n",
    "    date_to_query = yesterday\n",
    "\n",
    "    # Download raw text of article\n",
    "    url = f\"https://en.wikipedia.org/w/index.php?title={title}&action=raw\"\n",
    "    print(url)\n",
    "\n",
    "    article_text = requests.get(url).text\n",
    "\n",
    "    \n",
    "    print(f\"Token count: {num_tokens_from_string(article_text, 'cl100k_base')}\")\n",
    "\n",
    "\n",
    "    prompt = f\"Act as a professional news summarizer. Based on your knowledge of {title} and the following extract. In 1-2 sentences, explain why the {title} article might be trending on Wikipedia on #{date_to_query}:\\n\\n{article_text}\"\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=2048,\n",
    "    top_p=1\n",
    "    )\n",
    "    print(f\"response: {response}\")\n",
    "    print(f\"trendingreason: {response.choices[0].message.content}\")\n",
    "    \n",
    "    article={\n",
    "        'title': title,\n",
    "        'views': views,\n",
    "        'link': link,\n",
    "        'thumbnail': thumbnail,\n",
    "        'extract': extract,\n",
    "        'trendingreason': response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    articles_with_reasons.append(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kept running into rate limit errors... But the results I was getting are very positive and close to what I was looking for.\n",
    "\n",
    "On the downside.. running this for two days hit the Tokens Per Minute limits and cost almost $7....\n",
    "\n",
    "This is enough for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vital information to new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles_with_reasons saved to data/2024-12-28-trending-reasons.json\n"
     ]
    }
   ],
   "source": [
    "file_path = f'{file_directory}/{base_file_name}-trending-reasons.json'\n",
    "\n",
    "# Save to JSON file (overwrite if it already exists)\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(articles_with_reasons, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f'articles_with_reasons saved to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build HTML Page to display the top 10 list complete with thumbnails and the reason generated by Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\n",
       "    <li>\n",
       "        <h2>\n",
       "          <a href=\"https://en.wikipedia.org/wiki/Manmohan_Singh\" target=\"_blank\">Manmohan_Singh</a><br>\n",
       "        </h2>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Official_Portrait_of_the_Prime_Minister_Dr._Manmohan_Singh.jpg/320px-Official_Portrait_of_the_Prime_Minister_Dr._Manmohan_Singh.jpg\" alt=\"Thumbnail for Manmohan_Singh\"/><br>\n",
       "        <strong>Views:</strong> 889045<br>\n",
       "        <strong>Extract:</strong> Manmohan Singh was an Indian politician, economist, academic, and bureaucrat, who served as the 13th prime minister of India from 2004 to 2014. He was the fourth longest-serving prime minister after Jawaharlal Nehru, Indira Gandhi and Narendra Modi. A member of the Indian National Congress, Singh was the first Sikh prime minister of India. He was also the first prime minister since Nehru to be re-elected after completing a full five-year term.<br>\n",
       "        <strong>Reason for Trending:</strong> The article about Manmohan Singh, former Prime Minister of India, may be trending on Wikipedia due to his recent passing on December 26, 2024. His significant contributions as a key architect in liberalizing India's economy and his tenure as a respected global statesman are likely driving interest in his life and legacy.\n",
       "    </li>\n",
       "\n",
       "    \n",
       "    <li>\n",
       "        <h2>\n",
       "          <a href=\"https://en.wikipedia.org/wiki/Squid_Game_season_2\" target=\"_blank\">Squid_Game_season_2</a><br>\n",
       "        </h2>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/en/3/38/Squid_Game_season_2_poster.png\" alt=\"Thumbnail for Squid_Game_season_2\"/><br>\n",
       "        <strong>Views:</strong> 839241<br>\n",
       "        <strong>Extract:</strong> The second season of the South Korean dystopian survival thriller horror television series Squid Game, marketed as Squid Game 2, was created by Korean writer and television producer Hwang Dong-hyuk. Produced by Netflix, it was released on December 26, 2024.<br>\n",
       "        <strong>Reason for Trending:</strong> The \"Squid Game season 2\" article might be trending on Wikipedia due to its recent release on December 26, 2024, sparking interest as viewers seek more detailed insights into the latest episodes, character developments, and plot twists of this popular dystopian series. Additionally, anticipation may be high for the confirmed upcoming third and final season.\n",
       "    </li>\n",
       "\n",
       "    \n",
       "    <li>\n",
       "        <h2>\n",
       "          <a href=\"https://en.wikipedia.org/wiki/Nosferatu_(2024_film)\" target=\"_blank\">Nosferatu_(2024_film)</a><br>\n",
       "        </h2>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/en/4/48/Nosferatu_IMAX_poster_2024.jpg\" alt=\"Thumbnail for Nosferatu_(2024_film)\"/><br>\n",
       "        <strong>Views:</strong> 363577<br>\n",
       "        <strong>Extract:</strong> Nosferatu is a 2024 American gothic horror film written and directed by Robert Eggers. It is a remake of the 1922 silent film Nosferatu, which was inspired by Bram Stoker's 1897 novel Dracula. The film stars Bill Skarsgård as the titular character, Nicholas Hoult and Lily-Rose Depp as the married Hutter couple, with Aaron Taylor-Johnson, Emma Corrin, Ralph Ineson, Simon McBurney and Willem Dafoe in the supporting roles.<br>\n",
       "        <strong>Reason for Trending:</strong> The \"Nosferatu\" (2024) film article likely is trending on Wikipedia due to its recent release in the United States on December 25, 2024, and its positive reception with both critics and audiences, as part of its theatrical rollout. The notable direction by Robert Eggers, compelling performances by its cast, and innovative retelling of the classic horror story contribute to the heightened interest and discussions around the film.\n",
       "    </li>\n",
       "\n",
       "    \n",
       "    <li>\n",
       "        <h2>\n",
       "          <a href=\"https://en.wikipedia.org/wiki/Squid_Game\" target=\"_blank\">Squid_Game</a><br>\n",
       "        </h2>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Squid_Game_2021_vector_logo.svg/320px-Squid_Game_2021_vector_logo.svg.png\" alt=\"Thumbnail for Squid_Game\"/><br>\n",
       "        <strong>Views:</strong> 305070<br>\n",
       "        <strong>Extract:</strong> Squid Game is a South Korean dystopian survival thriller television series created, written and directed by Hwang Dong-hyuk for Netflix. The series revolves around a secret contest where 456 players, all of whom are in deep financial hardship, risk their lives to play a series of deadly children's games for the chance to win a ₩45.6 billion prize. The series' title draws from ojingŏ (\"squid\"), a Korean children's game. Lee Jung-jae leads an ensemble cast.<br>\n",
       "        <strong>Reason for Trending:</strong> The Squid Game article is likely trending on Wikipedia today due to the recent release of the show's second season on Netflix on December 26, 2024, which has attracted significant viewer interest and engagement. This widespread attention typically drives fans and new viewers to seek more detailed information about the series, its characters, and production details.\n",
       "    </li>\n",
       "\n",
       "    \n",
       "    <li>\n",
       "        <h2>\n",
       "          <a href=\"https://en.wikipedia.org/wiki/Ainsley_Earhardt\" target=\"_blank\">Ainsley_Earhardt</a><br>\n",
       "        </h2>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Ainsley_Earhardt_on_USS_New_York.jpg/320px-Ainsley_Earhardt_on_USS_New_York.jpg\" alt=\"Thumbnail for Ainsley_Earhardt\"/><br>\n",
       "        <strong>Views:</strong> 242629<br>\n",
       "        <strong>Extract:</strong> Ainsley Earhardt is an American conservative television host and author. She is a co-host of Fox & Friends.<br>\n",
       "        <strong>Reason for Trending:</strong> The Ainsley Earhardt article may be trending on Wikipedia due to the recent announcement of her engagement to fellow Fox News personality Sean Hannity on Christmas Day 2024, as highlighted in a significant update to her personal life details. This event has likely drawn increased public interest and media attention to her profile.\n",
       "    </li>\n",
       "\n",
       "    </ol>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start building the HTML\n",
    "html_list = \"<ol>\\n\"\n",
    "\n",
    "# Iterate through the data\n",
    "for item in articles_with_reasons:\n",
    "    title = item['title']\n",
    "    link = item['link']\n",
    "    thumbnail = item['thumbnail']\n",
    "    trendingreason = item['trendingreason']\n",
    "    views = item['views']\n",
    "    extract = item['extract']\n",
    "\n",
    "    # Handle null thumbnail\n",
    "    if thumbnail:\n",
    "        thumbnail_html = f'<img src=\"{thumbnail}\" alt=\"Thumbnail for {title}\"/><br>'\n",
    "    else:\n",
    "        thumbnail_html = '<p><em>No thumbnail available</em></p>'\n",
    "    \n",
    "    # Create a list item for each entry\n",
    "    html_list += f\"\"\"\n",
    "    <li>\n",
    "        <h2>\n",
    "          <a href=\"{link}\" target=\"_blank\">{title}</a><br>\n",
    "        </h2>\n",
    "        {thumbnail_html}\n",
    "        <strong>Views:</strong> {views}<br>\n",
    "        <strong>Extract:</strong> {extract}<br>\n",
    "        <strong>Reason for Trending:</strong> {trendingreason}\n",
    "    </li>\\n\n",
    "    \"\"\"\n",
    "\n",
    "# Close the HTML list\n",
    "html_list += \"</ol>\"\n",
    "\n",
    "# Save to html file (overwrite if it already exists)\n",
    "file_path = f'{file_directory}/{base_file_name}.html'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(html_list)\n",
    "\n",
    "# Display the HTML in the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(html_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
